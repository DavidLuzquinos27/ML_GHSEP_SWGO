import os
import time
import resource
from pathlib import Path
from functools import wraps
from joblib import Parallel, delayed
import pandas as pd
import numpy as np
import uproot
from tqdm import tqdm
import sys
import logging

# ---- CONFIGURACIÓN DE LOG ----
LOG_PATH = "/home/swgo_2024/RESULTADOS/missing_files.log"
logging.basicConfig(
    filename=LOG_PATH,
    filemode="a",                    # añadir al final
    level=logging.WARNING,           # solo avisos/errores
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logging.warning("TEST DE LOG")
# ==================== MEMORIA ========================

def limitar_memoria(max_mem_gb=240):
    """Limita el uso de memoria del proceso (Linux/macOS)."""
    soft, hard = resource.getrlimit(resource.RLIMIT_AS)
    resource.setrlimit(resource.RLIMIT_AS, (int(max_mem_gb * 1024**3), hard))

# ==================== DECORADORES ====================

def timed(func):
    """Decorator para medir el tiempo de ejecución de las funciones."""
    @wraps(func)
    def wrapper(*args, **kwargs):
        t0 = time.time()
        res = func(*args, **kwargs)
        print(f"[{func.__name__}] tiempo: {time.time() - t0:.2f}s")
        return res
    return wrapper

# ==================== CÁLCULO DE FEATURES ====================

@timed
def extract_basic_features(df_wh):
    """
    Calcula R, hits<=40, energía máxima fuera de 40, PICness y Rmax_aerie
    en un solo groupby-agg.
    """
    df = df_wh.copy()
    df['R'] = np.sqrt(df['HAWCSim.WH.XNE']**2 + df['HAWCSim.WH.YNE']**2) * 0.01
    
    df['R_shower'] = np.sqrt(
        (df['HAWCSim.WH.XNE']* 0.01 - df['rec.coreX'])**2 +
        (df['HAWCSim.WH.YNE']* 0.01 - df['rec.coreY'])**2
    ) 
    # df['hit40'] = df['R_shower'] <= 40

    # df['outer_charge'] = df['HAWCSim.WH.Energy'].where(~df['hit40'], 0.0)
    # df['valid_pic'] = df['HAWCSim.WH.Energy'] >= 0.1    #revisar
    def agg_funcs(g):
        
        # Compactness
        # Obtener coordenadas del core del evento
        x_core = g['rec.coreX'].iat[0]
        y_core = g['rec.coreY'].iat[0]

        # Calcular distancia desde el core (R_shower)
        g['R_shower'] = np.sqrt(
            (g['HAWCSim.WH.XNE']*0.01 - x_core)**2 +
            (g['HAWCSim.WH.YNE']*0.01 - y_core)**2
        )  # convertir a metros
        g['hit40'] = g['R_shower'] <= 40
        g['outer_charge'] = g['HAWCSim.Evt.nPE'].where(~g['hit40'], 0.0)
        # Compactness
        n_hit = g['hit40'].sum()
        cxpe_40 = g['outer_charge'].max() if g['outer_charge'].any() else 1e-6
        compact = n_hit / (1000 * cxpe_40)

        # PICness
        g['valid_pic'] = g['HAWCSim.WH.Energy'] >= 0.05
        mask_pic = g['valid_pic']

        r_vals_pic = g.loc[mask_pic, 'R'].values
        spread = (r_vals_pic.mean() / (r_vals_pic.std(ddof=1) + 1e-6)) if len(r_vals_pic) >= 2 else np.nan

        # Rmax_aerie
        #10^3^2=10^6=10^3
        #(10^3*10^-2)^2=10^2=10
        r_vals = np.sqrt(
            (g.loc[mask_pic, 'HAWCSim.WH.XNE']*0.01 - x_core)**2 +
            (g.loc[mask_pic, 'HAWCSim.WH.YNE']*0.01 - y_core)**2
        )
        rmax = r_vals.max() if len(r_vals) > 0 else np.nan

        
        # eventID (único por grupo)
        event_id = g['event.eventID'].iloc[0]
        


        return pd.Series({
            'event.eventID': event_id,
            'Compactness': compact,
            'PICness': spread,
            'Rmax_aerie': rmax
        })

    result = df.groupby('HAWCSim.Evt.Num', group_keys=False).apply(agg_funcs).reset_index()
    return result

@timed
def compute_NPEMC(df_pe):
    """Calcula NPEMC (número de PMTID impares por evento)."""
    odd = (
        df_pe['HAWCSim.PE.PMTID'].mod(2)
            .groupby(df_pe['HAWCSim.Evt.Num'])
            .sum()
            .rename('NPEMC')
    )
    return odd.reset_index()

# ==================== FUNCIONES AUXILIARES ====================

def load_parquets(pe_path, events_path, wh_path):
    # print(f"→ Leyendo parquet: {pe_path}")
    df_pe = pd.read_parquet(pe_path)
    # print(f"  columnas PE: {df_pe.columns.tolist()}")

    # print(f"→ Leyendo parquet: {events_path}")
    df_ev = pd.read_parquet(events_path)
    # print(f"  columnas Events: {df_ev.columns.tolist()}")

    # print(f"→ Leyendo parquet: {wh_path}")
    df_wh = pd.read_parquet(wh_path)
    # print(f"  columnas WH: {df_wh.columns.tolist()}")

    return df_pe, df_ev, df_wh

def load_root(path_reco):
    print(f"→ Leyendo ROOT: {path_reco}")
    up = uproot.open(path_reco)
    df_rc = up["XCDF;1"].arrays(library="pd")
    # print(f"  columnas Reco: {df_rc.columns.tolist()}")
    return df_rc

# ==================== PROCESAMIENTO POR BLOQUE ====================

@timed
def procesar_un_indice(suffix: str, particula: str, tipo: int, carpeta_salida: Path):
    print(f"\n=== Procesando suffix: {suffix} ===")
    
    df_pe_list, df_ev_list, df_wh_list, df_reco_list = [], [], [], []

    for bloque in range(5):
        base = f"DAT{suffix}_D8_{particula}_{bloque}_50000"
        pe_p = f"/home/swgo_2024/DATOS/M7_production_D8_{particula}_photoelectrons/hawcsim-{base}_photoelectrons.parquet"
        ev_p = f"/home/swgo_2024/DATOS/M7_production_D8_{particula}_events/hawcsim-{base}_events.parquet"
        wh_p = f"/home/swgo_2024/DATOS/M7_production_D8_{particula}_waterhits/hawcsim-{base}_waterhits.parquet"
        rc_p = f"/home/swgo_2024/DATOS/M7_reco_D8_{particula}_V2/reco-{base}.root"

        missing = [os.path.basename(p) for p in (pe_p, ev_p, wh_p, rc_p) if not os.path.exists(p)]
        if missing:
            msg = (f"Bloque incompleto -> particula='{particula}', suffix='{suffix}', "
                f"bloque={bloque}, faltan: {', '.join(missing)}")
            print("   > " + msg)
            logging.warning(msg)   # <- se guarda en el log
            continue

        pe, ev, wh = load_parquets(pe_p, ev_p, wh_p)
        try:
            rc = load_root(rc_p)
        except Exception as e:
            print(f"   [ERROR ROOT] {e}")
            continue

        df_pe_list.append(pe)
        df_ev_list.append(ev)
        df_wh_list.append(wh)
        df_reco_list.append(rc)

    if not (df_pe_list and df_ev_list and df_wh_list and df_reco_list):
        print("   ¡No hay datos completos para este suffix!")
        return

    # Concatenar
    df_pe     = pd.concat(df_pe_list, ignore_index=True).dropna().reset_index(drop=True)
    df_events = pd.concat(df_ev_list, ignore_index=True)
    df_wh     = pd.concat(df_wh_list, ignore_index=True)
    df_reco   = pd.concat(df_reco_list, ignore_index=True)
    # if suffix =='000002': print("df_reco keys:",df_reco.keys())
    
    # Filtrado y creación de índice para PE y WH
    df_pe = (
        df_pe[df_pe['HAWCSim.Evt.nWHit'] > 65] #filtro para eventos con más de 65 registros
        .assign(
            HAWCSim_WH_TankID=lambda d: (d['HAWCSim.PE.PMTID'] // 2).astype(int),
            Index=lambda d: d['HAWCSim.Evt.Num'].astype(str) + '_' +
                            (d['HAWCSim.PE.PMTID'] // 2).astype(int).astype(str)
        )
    )
        # Filtrado y creación de índice para WH
    df_wh = (
        df_wh[(df_wh['HAWCSim.Evt.nWHit'] > 65) & 
        (np.sqrt(df_wh['HAWCSim.WH.XNE']**2 + df_wh['HAWCSim.WH.YNE']**2) * 0.01 <= 560)] #filtro para eventos con más de 65 registros
        .assign(
            HAWCSim_WH_TankID=lambda d: (d['HAWCSim.PE.PMTID'] // 2).astype(int) if 'HAWCSim.PE.PMTID' in d else d['HAWCSim.WH.TankID'],
            Index=lambda d: d['HAWCSim.Evt.Num'].astype(str) + '_' + d['HAWCSim_WH_TankID'].astype(str)
        )
    )

    # print("Cuantiles de tiempo en PE")
    # Cuantiles de tiempo en PE
    # === Calcular la mediana de Time por tanque (Index) ===
    pe_grp = df_pe.groupby('Index').agg(
        Time=('HAWCSim.PE.Time', 'median'),
        EvtNum=('HAWCSim.Evt.Num', 'first')
    ).reset_index()

    # Normalizar Time por evento
    pe_grp['Time'] = pe_grp['Time'] - pe_grp.groupby('EvtNum')['Time'].transform('min')

    # Inicializar dicts para guardar por evento
    q_dict = {}
    std_dict = {}

    for q in range(5, 100, 10):
        # Agrupamos por evento para obtener las listas de Time por evento
        def calc_q_std(x):
            vals = x['Time'].values
            q_val = np.quantile(vals, q / 100)
            std = np.std(vals)
            return pd.Series({f'q_{q}': q_val, f'std_q{q}': std})

        q_stats = pe_grp.groupby('EvtNum').apply(calc_q_std)
        q_dict[q] = q_stats[f'q_{q}']
        std_dict[q] = q_stats[f'std_q{q}']

    # Unimos todo en un solo DataFrame
    df_q = pd.concat([q_dict[q] for q in range(5,100,10)] + [std_dict[q] for q in range(5,100,10)], axis=1).reset_index()

    
    # print("Coordenadas de core desde reco")
    # Coordenadas de core desde reco
    core_cols = ['event.eventID', 'mc.coreX', 'mc.coreY', 'rec.coreX', 'rec.coreY','rec.LHLatDistFitEnergy','rec.zenithAngle','mc.zenithAngle','rec.PINC']
    core = df_reco[core_cols].drop_duplicates('event.eventID')
    mask= np.sqrt(core['rec.coreX']**2 + core['rec.coreY']**2) <=580
    core = core[mask]




    # print("Features avanzadas")
    # print("suffix: ",   suffix)

    # Features avanzadas
    try:
        feats = extract_basic_features(
            df_wh.merge(core, left_on='HAWCSim.Evt.Num', right_on='event.eventID', how='inner')
        )
    except Exception as e:
        print(f"[ERROR extract_basic_features] Falló para particula='{particula}', suffix='{suffix}'")
        print(f"Motivo: {type(e).__name__}: {e}")
        sys.exit(1)
    npe = compute_NPEMC(df_pe)
    # print("Merge final y filtrado de energía y ángulos")
    # Merge final y filtrado de energía y ángulos
    try:
        df_final = (
            feats
            .merge(npe, on='HAWCSim.Evt.Num')
            .merge(df_q, left_on='HAWCSim.Evt.Num', right_on='EvtNum')
            .merge(df_events[['HAWCSim.Evt.Num', 'HAWCSim.Evt.Energy','HAWCSim.Evt.Theta']], on='HAWCSim.Evt.Num')
            .merge(core[['event.eventID', 'rec.LHLatDistFitEnergy']], on='event.eventID')
            .query('1 <= `rec.LHLatDistFitEnergy` <= 1e2 and `HAWCSim.Evt.Theta` <=30')
            .merge(core[['event.eventID', 'rec.zenithAngle', 'mc.zenithAngle','rec.PINC', 'rec.coreX', 'rec.coreY']], on='event.eventID')
            .assign(**{'rec.zenithAngle': lambda d: d['HAWCSim.Evt.Theta']})
            .sort_values('HAWCSim.Evt.Num')
            .drop_duplicates('HAWCSim.Evt.Num')
            .assign(Type_Particle=tipo)
        )
    except Exception as e:
        print(f"[ERROR merge_final] Falló para particula='{particula}', suffix='{suffix}'")
        print("feats",feats)
        print("df_reco",core.keys())
        print("df_wh",df_wh)
        print(f"Motivo: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return

    out = carpeta_salida / f"processed_{particula}_{suffix}.parquet"
    df_final.to_parquet(out)
    print(f"   → Guardado: {out} (shape: {df_final.shape})")

# ==================== PROCESAMIENTO EN PARALELO ====================

@timed
def procesar_archivos_parallel(particula: str, indices: range, tipo: int, carpeta_salida: Path):
    carpeta_salida.mkdir(parents=True, exist_ok=True)
    tiempos = []

    def wrapper(suffix: str):
        t0 = time.time()
        procesar_un_indice(suffix, particula, tipo, carpeta_salida)
        dur = time.time() - t0
        tiempos.append(dur)
        print(f"[{suffix}] {dur:.2f}s  | avg: {np.mean(tiempos):.2f}s  total: {np.sum(tiempos):.2f}s")

    Parallel(n_jobs=30)(
        delayed(wrapper)(str(i).zfill(6)) for i in tqdm(indices, desc=f"Paralelo {particula}")
    )

# ==================== CONCATENACIÓN FINAL ====================

@timed
def concatenar_resultados(ruta: Path, nombre_df: str):
    archivos = sorted(ruta.glob("processed_*.parquet"))
    print(f"Concatenando {len(archivos)} archivos en {ruta.name}...")
    # Filtra archivos no vacíos
    archivos_validos = [f for f in archivos if os.path.getsize(f) > 0]
    if not archivos_validos:
        print("No hay archivos válidos para concatenar")
        return None

    df = pd.concat((pd.read_parquet(f) for f in archivos_validos), ignore_index=True)
    destino = ruta.parent / f"{nombre_df}.parquet"
    df.to_parquet(destino)
    print(f"→ Guardado final: {destino} (shape: {df.shape})")
    return df

# ==================== BLOQUE PRINCIPAL ====================

if __name__ == "__main__":
    limitar_memoria(240)
    print("=== INICIO DEL PROCESAMIENTO ===")
    inicio_total = time.time()

    # gamma
    procesar_archivos_parallel(
        particula="gamma",
        indices=range(1, 4001),
        tipo=0, #proton:1 gamma:0, en el codigo de ML se intercambian 
        carpeta_salida=Path("/home/swgo_2024/RESULTADOS/gamma")
    )
    concatenar_resultados(
        Path("/home/swgo_2024/RESULTADOS/gamma"),
        "ML_gamma_v2"
    )


    # Proton (descomenta si lo necesitas)
    procesar_archivos_parallel(
        particula="proton",
        indices=range(1, 10001),
        tipo=1,
        carpeta_salida=Path("/home/swgo_2024/RESULTADOS/proton")
    )
    concatenar_resultados(
        Path("/home/swgo_2024/RESULTADOS/proton"),
        "ML_proton_v2"
    )

    fin_total = time.time()
    print(f"\n=== PROCESAMIENTO COMPLETADO en {fin_total - inicio_total:.2f}s ===")
